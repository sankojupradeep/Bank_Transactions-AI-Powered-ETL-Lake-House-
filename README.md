# ğŸ¦ Banking Data Lakehouse Pipeline

## ğŸ“Œ Project Overview  
This project implements a **Banking Data Lakehouse Pipeline** using **Apache Spark, Delta Lake, and Apache Airflow**.  
The goal is to design an **end-to-end ETL pipeline** that ingests raw banking transactions, applies transformations, and produces **fact and dimension tables** for analytical dashboards.

The architecture follows a **multi-layered lakehouse approach**:
- **Raw Layer** â†’ Generates synthetic transactional data  
- **Bronze Layer** â†’ Ingests raw CSV data into Delta tables  
- **Silver Layer** â†’ Cleans, standardizes, and enriches data  
- **Gold Layer** â†’ Builds fact & dimension tables for BI and reporting  

---

## âš™ï¸ Tools & Technologies  
- **Python 3.11** â€“ Data processing & scripting  
- **Apache Spark 4.0.1** â€“ Distributed data processing engine  
- **Delta Lake** â€“ ACID-compliant storage format  
- **Apache Airflow** â€“ Workflow orchestration & scheduling  
- **AWS S3** â€“ Cloud data lake storage  
- **Streamlit + Plotly** â€“ Interactive dashboards  
- **Docker / WSL2 (Ubuntu)** â€“ Development environment  




## ğŸ“‚ Project Structure  
```bash
Financial_Transaction Project/
â”‚
â”œâ”€â”€ data/                  # Raw data generator (raw_data.py)
â”œâ”€â”€ Bronze_layer/          # Ingestion scripts (csv_to_delta.py)
â”œâ”€â”€ Silver_layer/          # Data transformations (transformations.py)
â”œâ”€â”€ Gold_layer/            # Dimension & Fact table scripts (dimension.py, fact.py)
â”œâ”€â”€ LLM/                   # Streamlit dashboard & visualizations
â”œâ”€â”€ airflow_dags/          # Airflow DAG (banking_pipeline_orchestration.py)
â””â”€â”€ README.md              # Documentation

